{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n",
    "$$a^{(i)} = sigmoid(z^{(i)})\\tag{2}$$ \n",
    "$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n",
    "\n",
    "- You calculate the cost function by summing over all training examples:\n",
    "$$ cost = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})$$\n",
    "$cost = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})\\tag{4}$\n",
    "\n",
    "\n",
    "- Here is the gradient parameters expression\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{5}$$\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{6}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR(object):\n",
    "    ''' \n",
    "    A Logistic regression \n",
    "    Arguments:\n",
    "    input_dim -- number of input dimension \n",
    "    num_iterations -- number of iterations for training\n",
    "    learning_rate -- optimization learning rate parameter \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, input_dim, num_iterations, learning_rate):\n",
    "        \n",
    "        super(LR, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.num_iterations = num_iterations\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.W = None\n",
    "        self.b = None \n",
    "        \n",
    "        # initialize_with_zeros\n",
    "        \n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "        self.cost = None\n",
    "        self.m = None\n",
    "        \n",
    "    def initialize_with_zeros(self, dim):\n",
    "        ''' \n",
    "        returns a numpy ndarray of W(dim, 1) and b(0) \n",
    "        and assign it to self.W and self.b \n",
    "        '''\n",
    "        self.W = np.zeros((dim,1))\n",
    "        self.b = np.zeros(0)\n",
    "        \n",
    "    \n",
    "    def forward_prop(self, X, activation):\n",
    "        ''' returns sigmoid( (W.T x X) + b ) '''\n",
    "        \n",
    "        print('W:',self.W.shape,'X: ', X.shape)\n",
    "        Z = np.dot(self.W.T,X) + self.b\n",
    "        if activation == \"sigmoid\":\n",
    "            A = self.sigmoid(Z)\n",
    "       \n",
    "        return A\n",
    "    \n",
    "    \n",
    "    def back_prop(self, A, X, Y):\n",
    "        ''' \n",
    "        calculate and assign the parameters dw and db\n",
    "        '''\n",
    "        print('X in back:',X.shape)\n",
    "        amy = A - Y\n",
    "        self.dw = np.dot(X,(amy.T))/m\n",
    "        self.db = np.sum(amy, axis=1, keepdims=True)/self.m\n",
    "        \n",
    "        \n",
    "    def cost(self, A, Y):\n",
    "        ''' \n",
    "        returns the loss between A and Y \n",
    "        '''\n",
    "        print('Y cost:',Y.shape)\n",
    "        cost = -np.sum(np.multiply(Y, np.log(A)) + np.multiply(1-Y,np.log(1-A)))/self.m \n",
    "        \n",
    "        return cost\n",
    "    \n",
    "    def optimize(self):\n",
    "        ''' \n",
    "        update the W and b parameters using dw and db\n",
    "        '''\n",
    "        self.W -= self.learning_rate*self.dw\n",
    "        self.b -= self.learning_rate*self.db\n",
    "                         \n",
    "            \n",
    "    def predict(self, A):\n",
    "        '''\n",
    "        return the prediction of an input X using W and b\n",
    "        '''\n",
    "        prediction = (A > 0.5)\n",
    "    \n",
    "        return prediction\n",
    "        \n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        ''' \n",
    "        returns the sigmoid of z \n",
    "        '''\n",
    "        sigmoid = 1/(1+np.exp(-Z))\n",
    "        return sigmoid\n",
    "    \n",
    "    def train(self, X, Y, print_cost):\n",
    "        '''\n",
    "        initialize\n",
    "        \n",
    "        iterate:\n",
    "            forward_prop\n",
    "            cost (optional)\n",
    "            back_prop\n",
    "            optimize\n",
    "        '''\n",
    "        self.m = X.shape[1]\n",
    "        self.initialize_with_zeros(self.input_dim)\n",
    "        accuracy = 0.\n",
    "        for i in range(self.num_iterations):\n",
    "            A = self.forward_prop(X, activation=\"sigmoid\")\n",
    "            cost  = self.cost(A, Y)\n",
    "            self.back_prop(A, X, Y)\n",
    "            self.optimize()\n",
    "            pred = self.predict(A)\n",
    "            if print_cost == True:\n",
    "                print('cost after itertion ',i,':',cost)\n",
    "            if pred == Y:\n",
    "                accuracy += 1./self.m\n",
    "        print('Train accuracy:',accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
